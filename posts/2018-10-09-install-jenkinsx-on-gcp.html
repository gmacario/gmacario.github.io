<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><meta name="description" content="Gianpaolo Macario&#x27;s public rants"/><title>Installing Jenkins X on Google Cloud Platform</title><meta name="next-head-count" content="4"/><link rel="preload" href="/_next/static/css/08fafe92abad983f984c.css" as="style"/><link rel="stylesheet" href="/_next/static/css/08fafe92abad983f984c.css" data-n-g=""/><noscript data-n-css=""></noscript><link rel="preload" href="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" as="script"/><link rel="preload" href="/_next/static/chunks/framework.1a5e6c0bcaecf178eee2.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.a817edd9b2693fcc6af8.js" as="script"/><link rel="preload" href="/_next/static/chunks/main-daaff4f95c13923f49a5.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/_app-041b1432faef8bf5f62a.js" as="script"/><link rel="preload" href="/_next/static/chunks/b38cc82cec06c0ea8b096def674947a423bb2764.260eb1ac110a53311620.js" as="script"/><link rel="preload" href="/_next/static/chunks/pages/posts/%5Bslug%5D-1c9a30b06d2f3be2ac9c.js" as="script"/></head><body><div id="__next"><main class="h-screen flex flex-col"><header class="py-3 fixed top-0 z-50 flex flex-wrap justify-between w-screen bg-white border-b border-gray-200"><div class="mx-3 py-2 px-4  font-bold text-indigo-500">Gianpaolo Macario&#x27;s blog</div><div class="justify-end"><a class="mr-6 py-2 px-4 inline-block rounded h-full text-indigo-500 hover:text-white hover:bg-indigo-500" href="/">Home</a><a class="mr-6 py-2 px-4 inline-block rounded h-full text-indigo-500 hover:text-white hover:bg-indigo-500" href="/posts">Posts</a></div></header><div class="pt-28 mx-auto w-5/6 break-words flex flex-grow prose prose-indigo hover:prose-black md:prose-lg lg:prose-xl"><article><h1>Installing Jenkins X on Google Cloud Platform</h1><div class="prose prose-indigo hover:prose-black md:prose-lg lg:prose-xl"><!-- markdown-link-check-disable -->

<h3 id="introduction">Introduction</h3>
<p>Here are my notes while deploying Jenkins X on a Kubernetes cluster on GCP.</p>
<p>The commands in this page have been tested on host &quot;nemo&quot; (Ubuntu 18.04.1 LTS 64-bit).</p>
<h3 id="references">References</h3>
<ul>
<li><a href="https://jenkins-x.io/getting-started/">https://jenkins-x.io/getting-started/</a></li>
</ul>
<h3 id="create-cluster-on-gcp">Create cluster on GCP</h3>
<!-- 2018-10-09 09:47 CEST -->

<p>Reference: <a href="https://jenkins-x.io/getting-started/create-cluster/">https://jenkins-x.io/getting-started/create-cluster/</a></p>
<p>Browse Google Cloud Console at <a href="https://console.cloud.google.com/">https://console.cloud.google.com/</a></p>
<p>Select GCP project &quot;kubernetes-workshop-218213&quot;, then click on the &quot;Activate Cloud Shell&quot; icon.</p>
<p>Logged as <code>gmacario@cloudshell</code>, install the <code>jx</code> binary:</p>
<pre><code class="language-shell">mkdir -p ~/.jx/bin
curl -L https://github.com/jenkins-x/jx/releases/download/v1.3.414/jx-linux-amd64.tar.gz | tar xzv -C ~/.jx/bin
export PATH=$PATH:~/.jx/bin
echo &#39;export PATH=$PATH:~/.jx/bin&#39; &gt;&gt; ~/.bashrc
source ~/.bashrc
</code></pre>
<p>Now use the <a href="https://jenkins-x.io/commands/jx_create_cluster_gke">jx create cluster gke</a> command.</p>
<p>You can see a demo of this command here: <a href="https://jenkins-x.io/demos/create_cluster_gke/">https://jenkins-x.io/demos/create_cluster_gke/</a></p>
<pre><code class="language-shell">jx create cluster gke --skip-login
</code></pre>
<p>Result:</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx create cluster gke --skip-login
? Missing required dependencies, deselect to avoid auto installing: helm
Downloading https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz to /home/gmacario/.jx/bin/helm.tgz...
Downloaded /home/gmacario/.jx/bin/helm.tgz
Using helmBinary helm with feature flag: none
? Google Cloud Project: kubernetes-workshop-218213
Updated property [core/project].
Let&#39;s ensure we have container and compute enabled on your project via: gcloud services enable container compute
Operation &quot;operations/acf.e25f2f16-c2a5-41db-b5cb-3a93e2dac508&quot; finished successfully.
No cluster name provided so using a generated one: tonguetree
? Google Cloud Zone: europe-west1-b
? Google Cloud Machine Type: n1-standard-2
? Minimum number of Nodes 3
? Maximum number of Nodes 5
Creating cluster...Initialising cluster ...
Using helmBinary helm with feature flag: none
Storing the kubernetes provider gke in the TeamSettings
Updated the team settings in namespace jx
? Please enter the name you wish to use with git:  Gianpaolo Macario
? Please enter the email address you wish to use with git:  gmacario@gmail.com
Git configured for user: Gianpaolo Macario and email gmacario@gmail.com
Trying to create ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding for role: cluster-admin for user gmacario@gmail.com
: clusterrolebindings.rbac.authorization.k8s.io &quot;gmacario-gmail-com-cluster-admin-binding&quot; not foundCreated ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding
Using helm2
Configuring tiller
Created ServiceAccount tiller in namespace kube-system
Trying to create ClusterRoleBinding tiller for role: cluster-admin and ServiceAccount: kube-system/tiller
Created ClusterRoleBinding tiller
Initialising helm using ServiceAccount tiller in namespace kube-system
Using helmBinary helm with feature flag: none
helm installed and configured
? No existing ingress controller found in the kube-system namespace, shall we install one? Yes
Installing using helm binary: helm
Waiting for external loadbalancer to be created and update the nginx-ingress-controller service in kube-system namespace
Note: this loadbalancer will fail to be provisioned if you have insufficient quotas, this can happen easily on a GKE free account. To view quotas run: gcloud compute project-info describe
External loadbalancer created
Waiting to find the external host name of the ingress controller Service in namespace kube-system with name jxing-nginx-ingress-controller
You can now configure a wildcard DNS pointing to the new loadbalancer address 35.241.213.226

If you do not have a custom domain setup yet, Ingress rules will be set for magic dns nip.io.
Once you have a customer domain ready, you can update with the command jx upgrade ingress --cluster
If you don&#39;t have a wildcard DNS setup then setup a new CNAME and point it at: 35.241.213.226.nip.io then use the DNS domain in the next input...? Domain 35.241.213.226.nip.io
nginx ingress controller installed and configured
Lets set up a Git username and API token to be able to perform CI/CD

? GitHub user name: gmacario
To be able to create a repository on GitHub we need an API Token
Please click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo

Then COPY the token and enter in into the form below:

? API Token:
</code></pre>
<p>Browse <a href="https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo">https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo</a></p>
<p>then paste the API Token into the terminal</p>
<pre><code>? API Token: ****************************************
Updated the team settings in namespace jx
Cloning the Jenkins X cloud environments repo to /home/gmacario/.jx/cloud-environments
Enumerating objects: 44, done.
Counting objects: 100% (44/44), done.
Compressing objects: 100% (32/32), done.
Total 1001 (delta 19), reused 33 (delta 12), pack-reused 957
Generated helm values /home/gmacario/.jx/extraValues.yaml
Installing Jenkins X platform helm chart from: /home/gmacario/.jx/cloud-environments/env-gke
Installing jx into namespace jx
waiting for install to be ready, if this is the first time then it will take a while to download images
Jenkins X deployments ready in namespace jx


        ********************************************************

             NOTE: Your admin password is: xxxxxx

        ********************************************************


Getting Jenkins API Token
Using url http://jenkins.jx.35.241.213.226.nip.io/me/configure
Unable to automatically find API token with chromedp using URL http://jenkins.jx.35.241.213.226.nip.io/me/configure
Error: fork/exec /usr/bin/google-chrome: no such file or directory
Please go to http://jenkins.jx.35.241.213.226.nip.io/me/configure and click Show API Token to get your API Token
Then COPY the token and enter in into the form below:

? API Token:
</code></pre>
<!-- 2018-10-09 10:30 CEST -->

<p>Login to <a href="http://jenkins.jx.35.241.213.226.nip.io/me/configure">http://jenkins.jx.35.241.213.226.nip.io/me/configure</a> (username: admin; password: see above),
copy the API Token and paste it into the terminal</p>
<pre><code>? API Token: ********************************
Created user admin API Token for Jenkins server jenkins.jx.35.241.213.226.nip.io at http://jenkins.jx.35.241.213.226.nip.io
Updating Jenkins with new external URL details http://jenkins.jx.35.241.213.226.nip.io
Creating default staging and production environments
Using Git provider GitHub at https://github.com


About to create repository environment-tonguetree-staging on server https://github.com with user gmacario


Creating repository gmacario/environment-tonguetree-staging
Creating Git repository gmacario/environment-tonguetree-staging
Pushed Git repository to https://github.com/gmacario/environment-tonguetree-staging

Created environment staging
Created Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-staging/

Note that your first pipeline may take a few minutes to start while the necessary images get downloaded!

Creating GitHub webhook for gmacario/environment-tonguetree-staging for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/
Using Git provider GitHub at https://github.com


About to create repository environment-tonguetree-production on server https://github.com with user gmacario


Creating repository gmacario/environment-tonguetree-production
Creating Git repository gmacario/environment-tonguetree-production
Pushed Git repository to https://github.com/gmacario/environment-tonguetree-production

Created environment production
Created Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-production/

Note that your first pipeline may take a few minutes to start while the necessary images get downloaded!

Creating GitHub webhook for gmacario/environment-tonguetree-production for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/

Jenkins X installation completed successfully


        ********************************************************

             NOTE: Your admin password is: xxxx

        ********************************************************



Your Kubernetes context is now set to the namespace: jx
To switch back to your original namespace use: jx ns default
For help on switching contexts see: https://jenkins-x.io/developing/kube-context/

To import existing projects into Jenkins:       jx import
To create a new Spring Boot microservice:       jx create spring -d web -d actuator
To create a new microservice from a quickstart: jx create quickstart
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<h3 id="inspecting-the-products-of-jx-create-cluster-gke">Inspecting the products of <code>jx create cluster gke</code></h3>
<h4 id="kubernetes-cluster">Kubernetes cluster</h4>
<p>Logged as <code>gmacario@cloudshell</code></p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ kubectl cluster-info
Kubernetes master is running at https://35.195.217.164
GLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
Heapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy
KubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
Metrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<h4 id="others">Others</h4>
<ul>
<li>Jenkins instance: <a href="http://jenkins.jx.35.241.213.226.nip.io/">http://jenkins.jx.35.241.213.226.nip.io/</a></li>
<li>GitHub repository for staging environment: <a href="https://github.com/gmacario/environment-tonguetree-staging">https://github.com/gmacario/environment-tonguetree-staging</a></li>
<li>GitHub repository for production environment: <a href="https://github.com/gmacario/environment-tonguetree-production">https://github.com/gmacario/environment-tonguetree-production</a></li>
</ul>
<h4 id="run-jx-diagnose-as-gmacariocloudshell">Run <code>jx diagnose</code> as gmacario@cloudshell</h4>
<!-- 2018-10-09 11:39 CEST -->

<p>Logged in as <code>gmacario@cloudshell</code>, type the following command</p>
<pre><code class="language-shell">jx diagnose
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx diagnose
Running in namespace: jx
Jenkins X Version:
 Using helmBinary helm with feature flag: none
Failed to find helm installs: failed to run &#39;helm list&#39; command in directory &#39;&#39;, output: &#39;Error: incompatible versions client[v2.11.0] server[v2.10.0]&#39;: exit status 1
NAME               VERSION
jx                 1.3.399
Kubernetes cluster v1.9.7-gke.6
kubectl            v1.10.7
helm client        v2.11.0+g2e55dbe
helm server        v2.10.0+g9ad53aa
git                git version 2.11.0

Jenkins X Status:
 Jenkins X checks passed for Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m. Jenkins is running at http://jenkins.jx.35.241.213.226.nip.io

Kubernetes PVCs:
 NAME                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
jenkins                     Bound     pvc-4014d9f4-cb9c-11e8-a1b0-42010a84014f   30Gi       RWO            standard       1h
jenkins-x-chartmuseum       Bound     pvc-40120cba-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h
jenkins-x-docker-registry   Bound     pvc-40131c4c-cb9c-11e8-a1b0-42010a84014f   100Gi      RWO            standard       1h
jenkins-x-mongodb           Bound     pvc-4015b339-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h
jenkins-x-nexus             Bound     pvc-401782ce-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h

Kubernetes Pods:
 NAME                                            READY     STATUS    RESTARTS   AGE
jenkins-6d89bdd984-kgdrk                        1/1       Running   0          1h
jenkins-x-chartmuseum-645d78c798-9frf6          1/1       Running   0          1h
jenkins-x-controllerteam-858ff8c6b8-5vvjq       1/1       Running   0          1h
jenkins-x-controllerworkflow-6fcb699cd6-d4khj   1/1       Running   0          1h
jenkins-x-docker-registry-dcb6d6d44-dsz4n       1/1       Running   0          1h
jenkins-x-heapster-96bd95dcf-g27x4              2/2       Running   0          1h
jenkins-x-mongodb-968b595dd-rk5qs               1/1       Running   1          1h
jenkins-x-monocular-api-745c8dcd5f-kr6tg        1/1       Running   5          1h
jenkins-x-monocular-prerender-6d8897856-nlln5   1/1       Running   0          1h
jenkins-x-monocular-ui-7854f96776-njl6g         1/1       Running   0          1h
jenkins-x-nexus-55f87888dc-h5s4h                1/1       Running   0          1h

Kubernetes Ingresses:
 NAME              HOSTS                                      ADDRESS         PORTS     AGE
chartmuseum       chartmuseum.jx.35.241.213.226.nip.io       35.205.100.81   80        1h
docker-registry   docker-registry.jx.35.241.213.226.nip.io   35.205.100.81   80        1h
jenkins           jenkins.jx.35.241.213.226.nip.io           35.205.100.81   80        1h
monocular         monocular.jx.35.241.213.226.nip.io         35.205.100.81   80        1h
nexus             nexus.jx.35.241.213.226.nip.io             35.205.100.81   80        1h

Kubernetes Secrets:
 NAME                                       TYPE                                  DATA      AGE
cleanup-token-8dflb                        kubernetes.io/service-account-token   3         1h
default-token-qdwxp                        kubernetes.io/service-account-token   3         1h
expose-token-x8pt5                         kubernetes.io/service-account-token   3         1h
jenkins                                    Opaque                                3         1h
jenkins-docker-cfg                         Opaque                                1         1h
jenkins-git-credentials                    Opaque                                1         1h
jenkins-git-ssh                            Opaque                                2         1h
jenkins-hub-api-token                      Opaque                                1         1h
jenkins-maven-settings                     Opaque                                1         1h
jenkins-npm-token                          Opaque                                1         1h
jenkins-release-gpg                        Opaque                                4         1h
jenkins-ssh-config                         Opaque                                1         1h
jenkins-token-pt9cx                        kubernetes.io/service-account-token   3         1h
jenkins-x-chartmuseum                      Opaque                                2         1h
jenkins-x-controllerteam-token-zvn89       kubernetes.io/service-account-token   3         1h
jenkins-x-controllerworkflow-token-hwr86   kubernetes.io/service-account-token   3         1h
jenkins-x-docker-registry-secret           Opaque                                1         1h
jenkins-x-mongodb                          Opaque                                1         1h
jx-basic-auth                              Opaque                                1         1h
jx-install-config                          Opaque                                3         1h
jx-pipeline-git-github-github              Opaque                                2         1h
nexus                                      Opaque                                1         1h

Please visit https://jenkins-x.io/faq/issues/ for any known issues.
Finished printing diagnostic information.
gmacario@cloudshell:~ (kubernetes-workshop-218213)$:
</code></pre>
<h4 id="run-jx-compliance-as-gmacariocloudshell">Run <code>jx compliance</code> as gmacario@cloudshell</h4>
<!-- 2018-10-09 11:42 CEST -->

<p>Logged in as <code>gmacario@cloudshell</code>, type the following command</p>
<pre><code class="language-shell">jx compliance
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance
Run compliance tests against Kubernetes cluster

Available Commands:
  compliance delete  Deletes the Kubernetes resources allocated by the compliance tests
  compliance logs    Prints the logs of compliance tests
  compliance results Shows the results of compliance tests
  compliance run     Runs the compliance tests
  compliance status  Retrieves the status of compliance tests
Usage:
  jx compliance ACTION [flags] [options]
Use &quot;jx &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;jx options&quot; for a list of global command-line options (applies to all commands).
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<!-- 2018-10-09 11:44 CEST -->

<p>Command</p>
<pre><code class="language-shell">jx compliance run
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance run
INFO[0000] created object                                name=heptio-sonobuoy namespace= resource=namespaces
INFO[0000] created object                                name=sonobuoy-serviceaccount namespace=heptio-sonobuoy resource=serviceaccounts
INFO[0000] created object                                name=sonobuoy-serviceaccount-heptio-sonobuoy namespace= resource=clusterrolebindings
INFO[0000] created object                                name=sonobuoy-serviceaccount namespace= resource=clusterroles
INFO[0000] created object                                name=sonobuoy-config-cm namespace=heptio-sonobuoy resource=configmaps
INFO[0000] created object                                name=sonobuoy-plugins-cm namespace=heptio-sonobuoy resource=configmaps
INFO[0000] created object                                name=sonobuoy namespace=heptio-sonobuoy resource=pods
INFO[0000] created object                                name=sonobuoy-master namespace=heptio-sonobuoy resource=services
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<p>Command</p>
<pre><code class="language-shell">jx compliance status
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status
Compliance tests are still running, it can take up to 60 minutes.
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<!-- 2018-10-09 13:45 CEST -->

<p>After about 1h</p>
<pre><code class="language-shell">jx compliance status
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status
Compliance tests completed. Use `jx compliance results` to display the results.
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<p>Command</p>
<pre><code class="language-shell">jx compliance results
</code></pre>
<p>Result</p>
<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance results
STATUS TEST
FAILED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
PASSED [k8s.io] Docker Containers should be able to override the image&#39;s default arguments (docker cmd) [NodeConformance] [Conformance]
PASSED [k8s.io] Docker Containers should be able to override the image&#39;s default command (docker entrypoint) [NodeConformance] [Conformance]
PASSED [k8s.io] Docker Containers should be able to override the image&#39;s default command and arguments [NodeConformance] [Conformance]
PASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
PASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
PASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
PASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]
PASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
PASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]
PASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]
PASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]
PASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container should *not* be restarted with a exec &quot;cat /tmp/health&quot; liveness probe [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container should be restarted with a exec &quot;cat /tmp/health&quot; liveness probe [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]
PASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
PASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
PASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
PASSED [k8s.io] Variable Expansion should allow substituting values in a container&#39;s args [NodeConformance] [Conformance]
PASSED [k8s.io] Variable Expansion should allow substituting values in a container&#39;s command [NodeConformance] [Conformance]
PASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
PASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed  [Conformance]
PASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]
PASSED [sig-api-machinery] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
PASSED [sig-api-machinery] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
PASSED [sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
PASSED [sig-api-machinery] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that&#39;s waiting for dependents to be deleted [Conformance]
PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
PASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]
PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]
PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]
PASSED [sig-apps] Deployment deployment should support rollover [Conformance]
PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Conformance]
PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
PASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
PASSED [sig-network] DNS should provide DNS for services  [Conformance]
PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]
PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [NodeConformance] [Conformance]
PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [NodeConformance] [Conformance]
PASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]
PASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]
PASSED [sig-network] Services should provide secure master service  [Conformance]
PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]
PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]
PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
PASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide container&#39;s cpu limit [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide container&#39;s cpu request [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide container&#39;s memory limit [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide container&#39;s memory request [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should set DefaultMode on files [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should set mode on item file [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [NodeConformance] [Conformance]
PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]
PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [NodeConformance] [Conformance]
PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]
PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [NodeConformance] [Conformance]
...
PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
gmacario@cloudshell:~ (kubernetes-workshop-218213)$
</code></pre>
<h3 id="control-cluster-from-nemo">Control cluster from nemo</h3>
<p>Browse <a href="https://console.cloud.google.com">https://console.cloud.google.com</a> &gt; Kubernetes Engine &gt; Clusters</p>
<ul>
<li>Select cluster &quot;tonguetree&quot; &gt; Details<ul>
<li>Take note of Endpoint: 35.195.217.164</li>
</ul>
</li>
<li>Click &quot;Show cluster certificate&quot;<ul>
<li>Take note of Cluster CA certificate: ...</li>
</ul>
</li>
</ul>
<p>Reference: <a href="https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials">https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials</a></p>
<h4 id="list-kubernetes-clusters-on-gcp">List Kubernetes clusters on GCP</h4>
<p>Logged as <code>gpmacario@nemo</code></p>
<pre><code class="language-shell">gcloud container clusters list
</code></pre>
<p>Result:</p>
<pre><code>gpmacario@nemo:~ $ gcloud container clusters list
NAME        LOCATION        MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS
kube-101    europe-west1-b  1.10.7-gke.2    35.241.146.224  n1-standard-1  1.10.7-gke.2  2          RUNNING
tonguetree  europe-west1-b  1.9.7-gke.6     35.195.217.164  n1-standard-2  1.9.7-gke.6   3          RUNNING
gpmacario@nemo:~ $
</code></pre>
<h4 id="get-credentials-for-gke-cluster-tonguetree">Get credentials for GKE cluster &quot;tonguetree&quot;</h4>
<p>Logged as gpmacario@nemo</p>
<pre><code class="language-shell">gcloud container clusters get-credentials tonguetree
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ gcloud container clusters get-credentials tonguetree
Fetching cluster endpoint and auth data.
kubeconfig entry generated for tonguetree.
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-kubectl-cluster-info">run <code>kubectl cluster-info</code></h4>
<p>Command</p>
<pre><code class="language-shell">kubectl cluster-info
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ kubectl cluster-info
Kubernetes master is running at https://35.195.217.164
GLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
Heapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy
KubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
kubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy
Metrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy

To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-kubectl-get-nodes">Run <code>kubectl get nodes</code></h4>
<p>Command</p>
<pre><code class="language-shell">kubectl get nodes
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ kubectl get nodes
NAME                                        STATUS   ROLES    AGE   VERSION
gke-tonguetree-default-pool-5c0fe7ba-t50s   Ready    &lt;none&gt;   68m   v1.9.7-gke.6
gke-tonguetree-default-pool-5c0fe7ba-x7mw   Ready    &lt;none&gt;   68m   v1.9.7-gke.6
gke-tonguetree-default-pool-5c0fe7ba-xj5l   Ready    &lt;none&gt;   68m   v1.9.7-gke.6
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-kubectl-get-namespaces">Run <code>kubectl get namespaces</code></h4>
<p>Command</p>
<pre><code class="language-shell">kubectl get namespaces
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ kubectl get namespaces
NAME            STATUS   AGE
default         Active   75m
jx              Active   74m
jx-production   Active   52m
jx-staging      Active   52m
kube-public     Active   75m
kube-system     Active   75m
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-kubectl-get-services--n-jx">Run <code>kubectl get services -n jx</code></h4>
<p>Command</p>
<pre><code class="language-shell">kubectl get services -n jx
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ kubectl get services -n jx
NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
heapster                        ClusterIP   10.27.247.74    &lt;none&gt;        8082/TCP    63m
jenkins                         ClusterIP   10.27.249.0     &lt;none&gt;        8080/TCP    63m
jenkins-agent                   ClusterIP   10.27.253.40    &lt;none&gt;        50000/TCP   63m
jenkins-x-chartmuseum           ClusterIP   10.27.240.69    &lt;none&gt;        8080/TCP    63m
jenkins-x-docker-registry       ClusterIP   10.27.252.143   &lt;none&gt;        5000/TCP    63m
jenkins-x-mongodb               ClusterIP   10.27.251.85    &lt;none&gt;        27017/TCP   63m
jenkins-x-monocular-api         ClusterIP   10.27.241.218   &lt;none&gt;        80/TCP      63m
jenkins-x-monocular-prerender   ClusterIP   10.27.243.227   &lt;none&gt;        80/TCP      63m
jenkins-x-monocular-ui          ClusterIP   10.27.243.130   &lt;none&gt;        80/TCP      63m
nexus                           ClusterIP   10.27.243.12    &lt;none&gt;        80/TCP      63m
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-kubectl-proxy">Run <code>kubectl proxy</code></h4>
<p>Command</p>
<pre><code class="language-shell">kubectl proxy
</code></pre>
<pre><code>gpmacario@nemo:~ $ kubectl proxy
Starting to serve on 127.0.0.1:8001
</code></pre>
<h4 id="display-kubernetes-dashboard">Display Kubernetes dashboard</h4>
<p>Logged as <code>gpmacario@nemo</code>,
browse <a href="http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a></p>
<blockquote>
<p><strong>Kubeconfig</strong></p>
<p>Please select the kubeconfig file that you have created to configure access to the cluster.
To find out more about how to configure and use kubeconfig file, please refer to the
<a href="https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a> section.</p>
<p><strong>Token</strong></p>
<p>Every Service Account has a Secret with valid Bearer Token that can be used to log in to Dashboard.
To find out more about how to configure and use Bearer Tokens, please refer to the
<a href="https://kubernetes.io/docs/admin/authentication/">Authentication</a> section.</p>
</blockquote>
<p>Select &quot;Token&quot;, then paste the result of the following command</p>
<pre><code class="language-shell">kubectl config view | grep access-token
</code></pre>
<p>and click &quot;SIGN IN&quot;.</p>
<h3 id="trying-jx-commands-from-nemo">Trying <code>jx</code> commands from nemo</h3>
<h4 id="install-the-jx-binary">Install the <code>jx</code> binary</h4>
<p>Reference: <a href="https://jenkins-x.io/getting-started/install/">https://jenkins-x.io/getting-started/install/</a></p>
<p>Logged as gpmacario@nemo</p>
<pre><code class="language-shell">mkdir -p ~/Downloads &amp;&amp; cd ~/Downloads
curl -L https://github.com/jenkins-x/jx/releases/download/v1.3.399/jx-linux-amd64.tar.gz | tar xzv
sudo mv jx /usr/local/bin
</code></pre>
<h4 id="display-jx-help">Display jx help</h4>
<pre><code>gpmacario@nemo:~ $ jx


Installing:
  install                   Install Jenkins X in the current Kubernetes cluster
  uninstall                 Uninstall the Jenkins X platform
  upgrade                   Upgrades a resource
  create cluster            Create a new Kubernetes cluster
  update cluster            Updates an existing Kubernetes cluster
  create jenkins token      Adds a new username and API token for a Jenkins server
  init                      Init Jenkins X

Adding Projects to Jenkins X:
  import                    Imports a local project or Git repository into Jenkins
  create archetype          Create a new app from a Maven Archetype and import the generated code into Git and Jenkins for CI/CD
  create spring             Create a new Spring Boot application and import the generated code into Git and Jenkins for CI/CD
  create lile               Create a new Lile based application and import the generated code into Git and Jenkins for CI/CD
  create micro              Create a new micro based application and import the generated code into Git and Jenkins for CI/CD
  create quickstart         Create a new app from a Quickstart and import the generated code into Git and Jenkins for CI/CD
  create quickstartlocation Create a location of quickstarts for your team

Addons:
  create addon              Creates an addon
  create token addon        Adds a new token/login for a user for a given addon
  delete addon              Deletes one or more addons
  delete token addon        Deletes one or more API tokens for a user on an issue addon server

Git:
  create git server         Creates a new Git server URL


Working with CloudBees application:
  cloudbees                 Opens the CloudBees app for Kubernetes for visualising CI/CD and your environments
  login                     Onboard an user into the CloudBees application

Working with Environments:
  preview                   Creates or updates a Preview Environment for the current version of an application
  promote                   Promotes a version of an application to an Environment
  create environment        Create a new Environment which is used to promote your Team&#39;s Applications via Continuous Delivery
  delete environment        Deletes one or more Environments
  edit environment          Edits an Environment which is used to promote your Team&#39;s Applications via Continuous Delivery
  get environments          Display one or more Environments

Working with Jenkins X resources:
  get                       Display one or more resources
  edit                      Edit a resource
  create                    Create a new resource
  update                    Updates an existing resource
  delete                    Deletes one or more resources
  start                     Starts a process such as a pipeline
  stop                      Stops a process such as a pipeline

Jenkins X Pipeline Commands:
  step                      pipeline steps

Jenkins X services:
  controller                Runs a controller
  gc                        Garbage collects Jenkins X resources

Other Commands:
  diagnose                  Print diagnostic information about the Jenkins X installation
  docs                      Open the documentation in a browser
  help                      Help about any command
  version                   Print the version information
Options:
      --version=false: version for jx
Usage:
  jx [flags] [options]
Use &quot;jx &lt;command&gt; --help&quot; for more information about a given command.
Use &quot;jx options&quot; for a list of global command-line options (applies to all commands).
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-jx---version">Run <code>jx --version</code></h4>
<p>Command</p>
<pre><code class="language-shell">jx --version
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ jx --version
1.3.399
gpmacario@nemo:~ $
</code></pre>
<h4 id="run-jx-diagnose-as-gpmacarionemo">Run <code>jx diagnose</code> as gpmacario@nemo</h4>
<p>Command</p>
<pre><code class="language-shell">jx diagnose
</code></pre>
<p>Result</p>
<pre><code>gpmacario@nemo:~ $ jx diagnose
Running in namespace: default
Jenkins X Version:
 Using helmBinary helm with feature flag: none
NAME               VERSION
jx                 1.3.399
jenkins x platform 0.0.2755
Kubernetes cluster v1.9.7-gke.6
kubectl            v1.12.1
helm client        v2.9.1+g20adb27
helm server        v2.10.0+g9ad53aa
git                git version 2.17.1
Unable to get the Jenkins X Status
error: Command failed &#39;jx status&#39;: Unable to find JX components in Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m
you could try:   # Default installer which uses interactive prompts to generate git secrets
  jx install

  # Install with a GitHub personal access token
  jx install --git-username jenkins-x-bot --git-api-token 9fdbd2d070cd81eb12bca87861bcd850

  # If you know the cloud provider you can pass this as a CLI argument. E.g. for AWS
  jx install --provider=aws

Installs the Jenkins X platform on a Kubernetes cluster

Requires a --git-username and --git-api-token that can be used to create a new token. This is so the Jenkins X platform can git tag your releases

For more documentation see: https://jenkins-x.io/getting-started/install-on-cluster/

The current requirements are:

 *RBAC is enabled on the cluster

 *Insecure Docker registry is enabled for Docker registries running locally inside Kubernetes on the service IP range. See the above documentation for more detailerror: no deployments found in namespace default exit status 1
gpmacario@nemo:~ $
</code></pre>
<h3 id="summary">Summary</h3>
<p>This post explained how to set up a Jenkins X instance on Google Cloud Platform.</p>
<p>Stay tuned for future posts on the subject!</p>
<h3 id="see-also">See also</h3>
<ul>
<li><a href="https://jenkins-x.io/">https://jenkins-x.io/</a></li>
<li><a href="https://jenkins.io/projects/jenkins-x/">https://jenkins.io/projects/jenkins-x/</a></li>
</ul>
<!-- markdown-link-check-enable -->
<!-- EOF -->
</div></article></div><footer class="h-auto mt-20 bg-gray-100 justify-between border-t border-gray-200"><div class="pt-6 mx-auto  gap-x-6 w-9/12 flex flex-col prose prose-indigo hover:prose-black md:prose-lg lg:prose-xl"><div class="prose-xl"><p class="">gmacario.github.io</p></div><div class="flex flex-col sm:flex-row flex-wrap justify-between md:justify-start"><p class="mr-12 text-gray-600 prose-sm">gmacario.github.io</p><div class="mr-12 flex flex-col justify-start"><p class="text-gray-600 prose-sm flex flex-row"><svg class="MuiSvgIcon-root mr-2" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M12 .3a12 12 0 0 0-3.8 23.4c.6.1.8-.3.8-.6v-2c-3.3.7-4-1.6-4-1.6-.6-1.4-1.4-1.8-1.4-1.8-1-.7.1-.7.1-.7 1.2 0 1.9 1.2 1.9 1.2 1 1.8 2.8 1.3 3.5 1 0-.8.4-1.3.7-1.6-2.7-.3-5.5-1.3-5.5-6 0-1.2.5-2.3 1.3-3.1-.2-.4-.6-1.6 0-3.2 0 0 1-.3 3.4 1.2a11.5 11.5 0 0 1 6 0c2.3-1.5 3.3-1.2 3.3-1.2.6 1.6.2 2.8 0 3.2.9.8 1.3 1.9 1.3 3.2 0 4.6-2.8 5.6-5.5 5.9.5.4.9 1 .9 2.2v3.3c0 .3.1.7.8.6A12 12 0 0 0 12 .3"></path></svg><a href="https://github.com/gmacario">gmacario</a></p><p class="text-gray-600 prose-sm flex flex-row"><svg class="MuiSvgIcon-root mr-2" focusable="false" viewBox="0 0 24 24" aria-hidden="true"><path d="M22.46 6c-.77.35-1.6.58-2.46.69.88-.53 1.56-1.37 1.88-2.38-.83.5-1.75.85-2.72 1.05C18.37 4.5 17.26 4 16 4c-2.35 0-4.27 1.92-4.27 4.29 0 .34.04.67.11.98C8.28 9.09 5.11 7.38 3 4.79c-.37.63-.58 1.37-.58 2.15 0 1.49.75 2.81 1.91 3.56-.71 0-1.37-.2-1.95-.5v.03c0 2.08 1.48 3.82 3.44 4.21a4.22 4.22 0 0 1-1.93.07 4.28 4.28 0 0 0 4 2.98 8.521 8.521 0 0 1-5.33 1.84c-.34 0-.68-.02-1.02-.06C3.44 20.29 5.7 21 8.12 21 16 21 20.33 14.46 20.33 8.79c0-.19 0-.37-.01-.56.84-.6 1.56-1.36 2.14-2.23z"></path></svg><a href="https://www.twitter.com/gpmacario">gpmacario</a></p></div><p class="mr-12 text-gray-600 prose-sm">Gianpaolo Macario&#x27;s public rants</p></div></div></footer></main></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"title":"Installing Jenkins X on Google Cloud Platform","content":"\u003c!-- markdown-link-check-disable --\u003e\n\n\u003ch3 id=\"introduction\"\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eHere are my notes while deploying Jenkins X on a Kubernetes cluster on GCP.\u003c/p\u003e\n\u003cp\u003eThe commands in this page have been tested on host \u0026quot;nemo\u0026quot; (Ubuntu 18.04.1 LTS 64-bit).\u003c/p\u003e\n\u003ch3 id=\"references\"\u003eReferences\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://jenkins-x.io/getting-started/\"\u003ehttps://jenkins-x.io/getting-started/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"create-cluster-on-gcp\"\u003eCreate cluster on GCP\u003c/h3\u003e\n\u003c!-- 2018-10-09 09:47 CEST --\u003e\n\n\u003cp\u003eReference: \u003ca href=\"https://jenkins-x.io/getting-started/create-cluster/\"\u003ehttps://jenkins-x.io/getting-started/create-cluster/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBrowse Google Cloud Console at \u003ca href=\"https://console.cloud.google.com/\"\u003ehttps://console.cloud.google.com/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSelect GCP project \u0026quot;kubernetes-workshop-218213\u0026quot;, then click on the \u0026quot;Activate Cloud Shell\u0026quot; icon.\u003c/p\u003e\n\u003cp\u003eLogged as \u003ccode\u003egmacario@cloudshell\u003c/code\u003e, install the \u003ccode\u003ejx\u003c/code\u003e binary:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003emkdir -p ~/.jx/bin\ncurl -L https://github.com/jenkins-x/jx/releases/download/v1.3.414/jx-linux-amd64.tar.gz | tar xzv -C ~/.jx/bin\nexport PATH=$PATH:~/.jx/bin\necho \u0026#39;export PATH=$PATH:~/.jx/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc\nsource ~/.bashrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow use the \u003ca href=\"https://jenkins-x.io/commands/jx_create_cluster_gke\"\u003ejx create cluster gke\u003c/a\u003e command.\u003c/p\u003e\n\u003cp\u003eYou can see a demo of this command here: \u003ca href=\"https://jenkins-x.io/demos/create_cluster_gke/\"\u003ehttps://jenkins-x.io/demos/create_cluster_gke/\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx create cluster gke --skip-login\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx create cluster gke --skip-login\n? Missing required dependencies, deselect to avoid auto installing: helm\nDownloading https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz to /home/gmacario/.jx/bin/helm.tgz...\nDownloaded /home/gmacario/.jx/bin/helm.tgz\nUsing helmBinary helm with feature flag: none\n? Google Cloud Project: kubernetes-workshop-218213\nUpdated property [core/project].\nLet\u0026#39;s ensure we have container and compute enabled on your project via: gcloud services enable container compute\nOperation \u0026quot;operations/acf.e25f2f16-c2a5-41db-b5cb-3a93e2dac508\u0026quot; finished successfully.\nNo cluster name provided so using a generated one: tonguetree\n? Google Cloud Zone: europe-west1-b\n? Google Cloud Machine Type: n1-standard-2\n? Minimum number of Nodes 3\n? Maximum number of Nodes 5\nCreating cluster...Initialising cluster ...\nUsing helmBinary helm with feature flag: none\nStoring the kubernetes provider gke in the TeamSettings\nUpdated the team settings in namespace jx\n? Please enter the name you wish to use with git:  Gianpaolo Macario\n? Please enter the email address you wish to use with git:  gmacario@gmail.com\nGit configured for user: Gianpaolo Macario and email gmacario@gmail.com\nTrying to create ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding for role: cluster-admin for user gmacario@gmail.com\n: clusterrolebindings.rbac.authorization.k8s.io \u0026quot;gmacario-gmail-com-cluster-admin-binding\u0026quot; not foundCreated ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding\nUsing helm2\nConfiguring tiller\nCreated ServiceAccount tiller in namespace kube-system\nTrying to create ClusterRoleBinding tiller for role: cluster-admin and ServiceAccount: kube-system/tiller\nCreated ClusterRoleBinding tiller\nInitialising helm using ServiceAccount tiller in namespace kube-system\nUsing helmBinary helm with feature flag: none\nhelm installed and configured\n? No existing ingress controller found in the kube-system namespace, shall we install one? Yes\nInstalling using helm binary: helm\nWaiting for external loadbalancer to be created and update the nginx-ingress-controller service in kube-system namespace\nNote: this loadbalancer will fail to be provisioned if you have insufficient quotas, this can happen easily on a GKE free account. To view quotas run: gcloud compute project-info describe\nExternal loadbalancer created\nWaiting to find the external host name of the ingress controller Service in namespace kube-system with name jxing-nginx-ingress-controller\nYou can now configure a wildcard DNS pointing to the new loadbalancer address 35.241.213.226\n\nIf you do not have a custom domain setup yet, Ingress rules will be set for magic dns nip.io.\nOnce you have a customer domain ready, you can update with the command jx upgrade ingress --cluster\nIf you don\u0026#39;t have a wildcard DNS setup then setup a new CNAME and point it at: 35.241.213.226.nip.io then use the DNS domain in the next input...? Domain 35.241.213.226.nip.io\nnginx ingress controller installed and configured\nLets set up a Git username and API token to be able to perform CI/CD\n\n? GitHub user name: gmacario\nTo be able to create a repository on GitHub we need an API Token\nPlease click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo\n\nThen COPY the token and enter in into the form below:\n\n? API Token:\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBrowse \u003ca href=\"https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo\"\u003ehttps://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ethen paste the API Token into the terminal\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e? API Token: ****************************************\nUpdated the team settings in namespace jx\nCloning the Jenkins X cloud environments repo to /home/gmacario/.jx/cloud-environments\nEnumerating objects: 44, done.\nCounting objects: 100% (44/44), done.\nCompressing objects: 100% (32/32), done.\nTotal 1001 (delta 19), reused 33 (delta 12), pack-reused 957\nGenerated helm values /home/gmacario/.jx/extraValues.yaml\nInstalling Jenkins X platform helm chart from: /home/gmacario/.jx/cloud-environments/env-gke\nInstalling jx into namespace jx\nwaiting for install to be ready, if this is the first time then it will take a while to download images\nJenkins X deployments ready in namespace jx\n\n\n        ********************************************************\n\n             NOTE: Your admin password is: xxxxxx\n\n        ********************************************************\n\n\nGetting Jenkins API Token\nUsing url http://jenkins.jx.35.241.213.226.nip.io/me/configure\nUnable to automatically find API token with chromedp using URL http://jenkins.jx.35.241.213.226.nip.io/me/configure\nError: fork/exec /usr/bin/google-chrome: no such file or directory\nPlease go to http://jenkins.jx.35.241.213.226.nip.io/me/configure and click Show API Token to get your API Token\nThen COPY the token and enter in into the form below:\n\n? API Token:\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- 2018-10-09 10:30 CEST --\u003e\n\n\u003cp\u003eLogin to \u003ca href=\"http://jenkins.jx.35.241.213.226.nip.io/me/configure\"\u003ehttp://jenkins.jx.35.241.213.226.nip.io/me/configure\u003c/a\u003e (username: admin; password: see above),\ncopy the API Token and paste it into the terminal\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e? API Token: ********************************\nCreated user admin API Token for Jenkins server jenkins.jx.35.241.213.226.nip.io at http://jenkins.jx.35.241.213.226.nip.io\nUpdating Jenkins with new external URL details http://jenkins.jx.35.241.213.226.nip.io\nCreating default staging and production environments\nUsing Git provider GitHub at https://github.com\n\n\nAbout to create repository environment-tonguetree-staging on server https://github.com with user gmacario\n\n\nCreating repository gmacario/environment-tonguetree-staging\nCreating Git repository gmacario/environment-tonguetree-staging\nPushed Git repository to https://github.com/gmacario/environment-tonguetree-staging\n\nCreated environment staging\nCreated Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-staging/\n\nNote that your first pipeline may take a few minutes to start while the necessary images get downloaded!\n\nCreating GitHub webhook for gmacario/environment-tonguetree-staging for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/\nUsing Git provider GitHub at https://github.com\n\n\nAbout to create repository environment-tonguetree-production on server https://github.com with user gmacario\n\n\nCreating repository gmacario/environment-tonguetree-production\nCreating Git repository gmacario/environment-tonguetree-production\nPushed Git repository to https://github.com/gmacario/environment-tonguetree-production\n\nCreated environment production\nCreated Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-production/\n\nNote that your first pipeline may take a few minutes to start while the necessary images get downloaded!\n\nCreating GitHub webhook for gmacario/environment-tonguetree-production for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/\n\nJenkins X installation completed successfully\n\n\n        ********************************************************\n\n             NOTE: Your admin password is: xxxx\n\n        ********************************************************\n\n\n\nYour Kubernetes context is now set to the namespace: jx\nTo switch back to your original namespace use: jx ns default\nFor help on switching contexts see: https://jenkins-x.io/developing/kube-context/\n\nTo import existing projects into Jenkins:       jx import\nTo create a new Spring Boot microservice:       jx create spring -d web -d actuator\nTo create a new microservice from a quickstart: jx create quickstart\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"inspecting-the-products-of-jx-create-cluster-gke\"\u003eInspecting the products of \u003ccode\u003ejx create cluster gke\u003c/code\u003e\u003c/h3\u003e\n\u003ch4 id=\"kubernetes-cluster\"\u003eKubernetes cluster\u003c/h4\u003e\n\u003cp\u003eLogged as \u003ccode\u003egmacario@cloudshell\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ kubectl cluster-info\nKubernetes master is running at https://35.195.217.164\nGLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nkubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nMetrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"others\"\u003eOthers\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eJenkins instance: \u003ca href=\"http://jenkins.jx.35.241.213.226.nip.io/\"\u003ehttp://jenkins.jx.35.241.213.226.nip.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGitHub repository for staging environment: \u003ca href=\"https://github.com/gmacario/environment-tonguetree-staging\"\u003ehttps://github.com/gmacario/environment-tonguetree-staging\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eGitHub repository for production environment: \u003ca href=\"https://github.com/gmacario/environment-tonguetree-production\"\u003ehttps://github.com/gmacario/environment-tonguetree-production\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4 id=\"run-jx-diagnose-as-gmacariocloudshell\"\u003eRun \u003ccode\u003ejx diagnose\u003c/code\u003e as gmacario@cloudshell\u003c/h4\u003e\n\u003c!-- 2018-10-09 11:39 CEST --\u003e\n\n\u003cp\u003eLogged in as \u003ccode\u003egmacario@cloudshell\u003c/code\u003e, type the following command\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx diagnose\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx diagnose\nRunning in namespace: jx\nJenkins X Version:\n Using helmBinary helm with feature flag: none\nFailed to find helm installs: failed to run \u0026#39;helm list\u0026#39; command in directory \u0026#39;\u0026#39;, output: \u0026#39;Error: incompatible versions client[v2.11.0] server[v2.10.0]\u0026#39;: exit status 1\nNAME               VERSION\njx                 1.3.399\nKubernetes cluster v1.9.7-gke.6\nkubectl            v1.10.7\nhelm client        v2.11.0+g2e55dbe\nhelm server        v2.10.0+g9ad53aa\ngit                git version 2.11.0\n\nJenkins X Status:\n Jenkins X checks passed for Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m. Jenkins is running at http://jenkins.jx.35.241.213.226.nip.io\n\nKubernetes PVCs:\n NAME                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\njenkins                     Bound     pvc-4014d9f4-cb9c-11e8-a1b0-42010a84014f   30Gi       RWO            standard       1h\njenkins-x-chartmuseum       Bound     pvc-40120cba-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\njenkins-x-docker-registry   Bound     pvc-40131c4c-cb9c-11e8-a1b0-42010a84014f   100Gi      RWO            standard       1h\njenkins-x-mongodb           Bound     pvc-4015b339-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\njenkins-x-nexus             Bound     pvc-401782ce-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\n\nKubernetes Pods:\n NAME                                            READY     STATUS    RESTARTS   AGE\njenkins-6d89bdd984-kgdrk                        1/1       Running   0          1h\njenkins-x-chartmuseum-645d78c798-9frf6          1/1       Running   0          1h\njenkins-x-controllerteam-858ff8c6b8-5vvjq       1/1       Running   0          1h\njenkins-x-controllerworkflow-6fcb699cd6-d4khj   1/1       Running   0          1h\njenkins-x-docker-registry-dcb6d6d44-dsz4n       1/1       Running   0          1h\njenkins-x-heapster-96bd95dcf-g27x4              2/2       Running   0          1h\njenkins-x-mongodb-968b595dd-rk5qs               1/1       Running   1          1h\njenkins-x-monocular-api-745c8dcd5f-kr6tg        1/1       Running   5          1h\njenkins-x-monocular-prerender-6d8897856-nlln5   1/1       Running   0          1h\njenkins-x-monocular-ui-7854f96776-njl6g         1/1       Running   0          1h\njenkins-x-nexus-55f87888dc-h5s4h                1/1       Running   0          1h\n\nKubernetes Ingresses:\n NAME              HOSTS                                      ADDRESS         PORTS     AGE\nchartmuseum       chartmuseum.jx.35.241.213.226.nip.io       35.205.100.81   80        1h\ndocker-registry   docker-registry.jx.35.241.213.226.nip.io   35.205.100.81   80        1h\njenkins           jenkins.jx.35.241.213.226.nip.io           35.205.100.81   80        1h\nmonocular         monocular.jx.35.241.213.226.nip.io         35.205.100.81   80        1h\nnexus             nexus.jx.35.241.213.226.nip.io             35.205.100.81   80        1h\n\nKubernetes Secrets:\n NAME                                       TYPE                                  DATA      AGE\ncleanup-token-8dflb                        kubernetes.io/service-account-token   3         1h\ndefault-token-qdwxp                        kubernetes.io/service-account-token   3         1h\nexpose-token-x8pt5                         kubernetes.io/service-account-token   3         1h\njenkins                                    Opaque                                3         1h\njenkins-docker-cfg                         Opaque                                1         1h\njenkins-git-credentials                    Opaque                                1         1h\njenkins-git-ssh                            Opaque                                2         1h\njenkins-hub-api-token                      Opaque                                1         1h\njenkins-maven-settings                     Opaque                                1         1h\njenkins-npm-token                          Opaque                                1         1h\njenkins-release-gpg                        Opaque                                4         1h\njenkins-ssh-config                         Opaque                                1         1h\njenkins-token-pt9cx                        kubernetes.io/service-account-token   3         1h\njenkins-x-chartmuseum                      Opaque                                2         1h\njenkins-x-controllerteam-token-zvn89       kubernetes.io/service-account-token   3         1h\njenkins-x-controllerworkflow-token-hwr86   kubernetes.io/service-account-token   3         1h\njenkins-x-docker-registry-secret           Opaque                                1         1h\njenkins-x-mongodb                          Opaque                                1         1h\njx-basic-auth                              Opaque                                1         1h\njx-install-config                          Opaque                                3         1h\njx-pipeline-git-github-github              Opaque                                2         1h\nnexus                                      Opaque                                1         1h\n\nPlease visit https://jenkins-x.io/faq/issues/ for any known issues.\nFinished printing diagnostic information.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$:\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-jx-compliance-as-gmacariocloudshell\"\u003eRun \u003ccode\u003ejx compliance\u003c/code\u003e as gmacario@cloudshell\u003c/h4\u003e\n\u003c!-- 2018-10-09 11:42 CEST --\u003e\n\n\u003cp\u003eLogged in as \u003ccode\u003egmacario@cloudshell\u003c/code\u003e, type the following command\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx compliance\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance\nRun compliance tests against Kubernetes cluster\n\nAvailable Commands:\n  compliance delete  Deletes the Kubernetes resources allocated by the compliance tests\n  compliance logs    Prints the logs of compliance tests\n  compliance results Shows the results of compliance tests\n  compliance run     Runs the compliance tests\n  compliance status  Retrieves the status of compliance tests\nUsage:\n  jx compliance ACTION [flags] [options]\nUse \u0026quot;jx \u0026lt;command\u0026gt; --help\u0026quot; for more information about a given command.\nUse \u0026quot;jx options\u0026quot; for a list of global command-line options (applies to all commands).\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- 2018-10-09 11:44 CEST --\u003e\n\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx compliance run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance run\nINFO[0000] created object                                name=heptio-sonobuoy namespace= resource=namespaces\nINFO[0000] created object                                name=sonobuoy-serviceaccount namespace=heptio-sonobuoy resource=serviceaccounts\nINFO[0000] created object                                name=sonobuoy-serviceaccount-heptio-sonobuoy namespace= resource=clusterrolebindings\nINFO[0000] created object                                name=sonobuoy-serviceaccount namespace= resource=clusterroles\nINFO[0000] created object                                name=sonobuoy-config-cm namespace=heptio-sonobuoy resource=configmaps\nINFO[0000] created object                                name=sonobuoy-plugins-cm namespace=heptio-sonobuoy resource=configmaps\nINFO[0000] created object                                name=sonobuoy namespace=heptio-sonobuoy resource=pods\nINFO[0000] created object                                name=sonobuoy-master namespace=heptio-sonobuoy resource=services\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx compliance status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status\nCompliance tests are still running, it can take up to 60 minutes.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003c!-- 2018-10-09 13:45 CEST --\u003e\n\n\u003cp\u003eAfter about 1h\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx compliance status\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status\nCompliance tests completed. Use `jx compliance results` to display the results.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx compliance results\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance results\nSTATUS TEST\nFAILED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image\u0026#39;s default arguments (docker cmd) [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image\u0026#39;s default command (docker entrypoint) [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image\u0026#39;s default command and arguments [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]\nPASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should *not* be restarted with a exec \u0026quot;cat /tmp/health\u0026quot; liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should be restarted with a exec \u0026quot;cat /tmp/health\u0026quot; liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow substituting values in a container\u0026#39;s args [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow substituting values in a container\u0026#39;s command [NodeConformance] [Conformance]\nPASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]\nPASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed  [Conformance]\nPASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]\nPASSED [sig-api-machinery] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]\nPASSED [sig-api-machinery] Downward API should provide container\u0026#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide host IP as an env var [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]\nPASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]\nPASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]\nPASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]\nPASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that\u0026#39;s waiting for dependents to be deleted [Conformance]\nPASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]\nPASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]\nPASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]\nPASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]\nPASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]\nPASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]\nPASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]\nPASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]\nPASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]\nPASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]\nPASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]\nPASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]\nPASSED [sig-apps] Deployment deployment should support rollover [Conformance]\nPASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]\nPASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]\nPASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]\nPASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]\nPASSED [sig-network] DNS should provide DNS for services  [Conformance]\nPASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [NodeConformance] [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]\nPASSED [sig-network] Service endpoints latency should not be very high  [Conformance]\nPASSED [sig-network] Services should provide secure master service  [Conformance]\nPASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]\nPASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container\u0026#39;s cpu limit [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container\u0026#39;s cpu request [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container\u0026#39;s memory limit [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container\u0026#39;s memory request [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should set DefaultMode on files [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should set mode on item file [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [NodeConformance] [Conformance]\n...\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"control-cluster-from-nemo\"\u003eControl cluster from nemo\u003c/h3\u003e\n\u003cp\u003eBrowse \u003ca href=\"https://console.cloud.google.com\"\u003ehttps://console.cloud.google.com\u003c/a\u003e \u0026gt; Kubernetes Engine \u0026gt; Clusters\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSelect cluster \u0026quot;tonguetree\u0026quot; \u0026gt; Details\u003cul\u003e\n\u003cli\u003eTake note of Endpoint: 35.195.217.164\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eClick \u0026quot;Show cluster certificate\u0026quot;\u003cul\u003e\n\u003cli\u003eTake note of Cluster CA certificate: ...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eReference: \u003ca href=\"https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials\"\u003ehttps://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials\u003c/a\u003e\u003c/p\u003e\n\u003ch4 id=\"list-kubernetes-clusters-on-gcp\"\u003eList Kubernetes clusters on GCP\u003c/h4\u003e\n\u003cp\u003eLogged as \u003ccode\u003egpmacario@nemo\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003egcloud container clusters list\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ gcloud container clusters list\nNAME        LOCATION        MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\nkube-101    europe-west1-b  1.10.7-gke.2    35.241.146.224  n1-standard-1  1.10.7-gke.2  2          RUNNING\ntonguetree  europe-west1-b  1.9.7-gke.6     35.195.217.164  n1-standard-2  1.9.7-gke.6   3          RUNNING\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"get-credentials-for-gke-cluster-tonguetree\"\u003eGet credentials for GKE cluster \u0026quot;tonguetree\u0026quot;\u003c/h4\u003e\n\u003cp\u003eLogged as gpmacario@nemo\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003egcloud container clusters get-credentials tonguetree\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ gcloud container clusters get-credentials tonguetree\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for tonguetree.\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-kubectl-cluster-info\"\u003erun \u003ccode\u003ekubectl cluster-info\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl cluster-info\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ kubectl cluster-info\nKubernetes master is running at https://35.195.217.164\nGLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nkubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nMetrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-kubectl-get-nodes\"\u003eRun \u003ccode\u003ekubectl get nodes\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl get nodes\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE   VERSION\ngke-tonguetree-default-pool-5c0fe7ba-t50s   Ready    \u0026lt;none\u0026gt;   68m   v1.9.7-gke.6\ngke-tonguetree-default-pool-5c0fe7ba-x7mw   Ready    \u0026lt;none\u0026gt;   68m   v1.9.7-gke.6\ngke-tonguetree-default-pool-5c0fe7ba-xj5l   Ready    \u0026lt;none\u0026gt;   68m   v1.9.7-gke.6\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-kubectl-get-namespaces\"\u003eRun \u003ccode\u003ekubectl get namespaces\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl get namespaces\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ kubectl get namespaces\nNAME            STATUS   AGE\ndefault         Active   75m\njx              Active   74m\njx-production   Active   52m\njx-staging      Active   52m\nkube-public     Active   75m\nkube-system     Active   75m\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-kubectl-get-services--n-jx\"\u003eRun \u003ccode\u003ekubectl get services -n jx\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl get services -n jx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ kubectl get services -n jx\nNAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE\nheapster                        ClusterIP   10.27.247.74    \u0026lt;none\u0026gt;        8082/TCP    63m\njenkins                         ClusterIP   10.27.249.0     \u0026lt;none\u0026gt;        8080/TCP    63m\njenkins-agent                   ClusterIP   10.27.253.40    \u0026lt;none\u0026gt;        50000/TCP   63m\njenkins-x-chartmuseum           ClusterIP   10.27.240.69    \u0026lt;none\u0026gt;        8080/TCP    63m\njenkins-x-docker-registry       ClusterIP   10.27.252.143   \u0026lt;none\u0026gt;        5000/TCP    63m\njenkins-x-mongodb               ClusterIP   10.27.251.85    \u0026lt;none\u0026gt;        27017/TCP   63m\njenkins-x-monocular-api         ClusterIP   10.27.241.218   \u0026lt;none\u0026gt;        80/TCP      63m\njenkins-x-monocular-prerender   ClusterIP   10.27.243.227   \u0026lt;none\u0026gt;        80/TCP      63m\njenkins-x-monocular-ui          ClusterIP   10.27.243.130   \u0026lt;none\u0026gt;        80/TCP      63m\nnexus                           ClusterIP   10.27.243.12    \u0026lt;none\u0026gt;        80/TCP      63m\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-kubectl-proxy\"\u003eRun \u003ccode\u003ekubectl proxy\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl proxy\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ kubectl proxy\nStarting to serve on 127.0.0.1:8001\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"display-kubernetes-dashboard\"\u003eDisplay Kubernetes dashboard\u003c/h4\u003e\n\u003cp\u003eLogged as \u003ccode\u003egpmacario@nemo\u003c/code\u003e,\nbrowse \u003ca href=\"http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\"\u003ehttp://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eKubeconfig\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePlease select the kubeconfig file that you have created to configure access to the cluster.\nTo find out more about how to configure and use kubeconfig file, please refer to the\n\u003ca href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\"\u003eConfigure Access to Multiple Clusters\u003c/a\u003e section.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eToken\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eEvery Service Account has a Secret with valid Bearer Token that can be used to log in to Dashboard.\nTo find out more about how to configure and use Bearer Tokens, please refer to the\n\u003ca href=\"https://kubernetes.io/docs/admin/authentication/\"\u003eAuthentication\u003c/a\u003e section.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eSelect \u0026quot;Token\u0026quot;, then paste the result of the following command\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ekubectl config view | grep access-token\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand click \u0026quot;SIGN IN\u0026quot;.\u003c/p\u003e\n\u003ch3 id=\"trying-jx-commands-from-nemo\"\u003eTrying \u003ccode\u003ejx\u003c/code\u003e commands from nemo\u003c/h3\u003e\n\u003ch4 id=\"install-the-jx-binary\"\u003eInstall the \u003ccode\u003ejx\u003c/code\u003e binary\u003c/h4\u003e\n\u003cp\u003eReference: \u003ca href=\"https://jenkins-x.io/getting-started/install/\"\u003ehttps://jenkins-x.io/getting-started/install/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eLogged as gpmacario@nemo\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003emkdir -p ~/Downloads \u0026amp;\u0026amp; cd ~/Downloads\ncurl -L https://github.com/jenkins-x/jx/releases/download/v1.3.399/jx-linux-amd64.tar.gz | tar xzv\nsudo mv jx /usr/local/bin\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"display-jx-help\"\u003eDisplay jx help\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ jx\n\n\nInstalling:\n  install                   Install Jenkins X in the current Kubernetes cluster\n  uninstall                 Uninstall the Jenkins X platform\n  upgrade                   Upgrades a resource\n  create cluster            Create a new Kubernetes cluster\n  update cluster            Updates an existing Kubernetes cluster\n  create jenkins token      Adds a new username and API token for a Jenkins server\n  init                      Init Jenkins X\n\nAdding Projects to Jenkins X:\n  import                    Imports a local project or Git repository into Jenkins\n  create archetype          Create a new app from a Maven Archetype and import the generated code into Git and Jenkins for CI/CD\n  create spring             Create a new Spring Boot application and import the generated code into Git and Jenkins for CI/CD\n  create lile               Create a new Lile based application and import the generated code into Git and Jenkins for CI/CD\n  create micro              Create a new micro based application and import the generated code into Git and Jenkins for CI/CD\n  create quickstart         Create a new app from a Quickstart and import the generated code into Git and Jenkins for CI/CD\n  create quickstartlocation Create a location of quickstarts for your team\n\nAddons:\n  create addon              Creates an addon\n  create token addon        Adds a new token/login for a user for a given addon\n  delete addon              Deletes one or more addons\n  delete token addon        Deletes one or more API tokens for a user on an issue addon server\n\nGit:\n  create git server         Creates a new Git server URL\n\n\nWorking with CloudBees application:\n  cloudbees                 Opens the CloudBees app for Kubernetes for visualising CI/CD and your environments\n  login                     Onboard an user into the CloudBees application\n\nWorking with Environments:\n  preview                   Creates or updates a Preview Environment for the current version of an application\n  promote                   Promotes a version of an application to an Environment\n  create environment        Create a new Environment which is used to promote your Team\u0026#39;s Applications via Continuous Delivery\n  delete environment        Deletes one or more Environments\n  edit environment          Edits an Environment which is used to promote your Team\u0026#39;s Applications via Continuous Delivery\n  get environments          Display one or more Environments\n\nWorking with Jenkins X resources:\n  get                       Display one or more resources\n  edit                      Edit a resource\n  create                    Create a new resource\n  update                    Updates an existing resource\n  delete                    Deletes one or more resources\n  start                     Starts a process such as a pipeline\n  stop                      Stops a process such as a pipeline\n\nJenkins X Pipeline Commands:\n  step                      pipeline steps\n\nJenkins X services:\n  controller                Runs a controller\n  gc                        Garbage collects Jenkins X resources\n\nOther Commands:\n  diagnose                  Print diagnostic information about the Jenkins X installation\n  docs                      Open the documentation in a browser\n  help                      Help about any command\n  version                   Print the version information\nOptions:\n      --version=false: version for jx\nUsage:\n  jx [flags] [options]\nUse \u0026quot;jx \u0026lt;command\u0026gt; --help\u0026quot; for more information about a given command.\nUse \u0026quot;jx options\u0026quot; for a list of global command-line options (applies to all commands).\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-jx---version\"\u003eRun \u003ccode\u003ejx --version\u003c/code\u003e\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx --version\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ jx --version\n1.3.399\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4 id=\"run-jx-diagnose-as-gpmacarionemo\"\u003eRun \u003ccode\u003ejx diagnose\u003c/code\u003e as gpmacario@nemo\u003c/h4\u003e\n\u003cp\u003eCommand\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-shell\"\u003ejx diagnose\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eResult\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egpmacario@nemo:~ $ jx diagnose\nRunning in namespace: default\nJenkins X Version:\n Using helmBinary helm with feature flag: none\nNAME               VERSION\njx                 1.3.399\njenkins x platform 0.0.2755\nKubernetes cluster v1.9.7-gke.6\nkubectl            v1.12.1\nhelm client        v2.9.1+g20adb27\nhelm server        v2.10.0+g9ad53aa\ngit                git version 2.17.1\nUnable to get the Jenkins X Status\nerror: Command failed \u0026#39;jx status\u0026#39;: Unable to find JX components in Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m\nyou could try:   # Default installer which uses interactive prompts to generate git secrets\n  jx install\n\n  # Install with a GitHub personal access token\n  jx install --git-username jenkins-x-bot --git-api-token 9fdbd2d070cd81eb12bca87861bcd850\n\n  # If you know the cloud provider you can pass this as a CLI argument. E.g. for AWS\n  jx install --provider=aws\n\nInstalls the Jenkins X platform on a Kubernetes cluster\n\nRequires a --git-username and --git-api-token that can be used to create a new token. This is so the Jenkins X platform can git tag your releases\n\nFor more documentation see: https://jenkins-x.io/getting-started/install-on-cluster/\n\nThe current requirements are:\n\n *RBAC is enabled on the cluster\n\n *Insecure Docker registry is enabled for Docker registries running locally inside Kubernetes on the service IP range. See the above documentation for more detailerror: no deployments found in namespace default exit status 1\ngpmacario@nemo:~ $\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"summary\"\u003eSummary\u003c/h3\u003e\n\u003cp\u003eThis post explained how to set up a Jenkins X instance on Google Cloud Platform.\u003c/p\u003e\n\u003cp\u003eStay tuned for future posts on the subject!\u003c/p\u003e\n\u003ch3 id=\"see-also\"\u003eSee also\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://jenkins-x.io/\"\u003ehttps://jenkins-x.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://jenkins.io/projects/jenkins-x/\"\u003ehttps://jenkins.io/projects/jenkins-x/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- markdown-link-check-enable --\u003e\n\u003c!-- EOF --\u003e\n"},"config":{"title":"gmacario.github.io","description":"Gianpaolo Macario's public rants","social":{"github":{"name":"gmacario","link":"https://github.com/gmacario"},"twitter":{"name":"gpmacario","link":"https://www.twitter.com/gpmacario"}}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"2018-10-09-install-jenkinsx-on-gcp"},"buildId":"UBjnWsN6EELCOa3hLp-cA","runtimeConfig":{},"isFallback":false,"gsp":true}</script><script nomodule="" src="/_next/static/chunks/polyfills-144e5fa6fafab6397d9c.js"></script><script src="/_next/static/chunks/webpack-50bee04d1dc61f8adf5b.js" async=""></script><script src="/_next/static/chunks/framework.1a5e6c0bcaecf178eee2.js" async=""></script><script src="/_next/static/chunks/commons.a817edd9b2693fcc6af8.js" async=""></script><script src="/_next/static/chunks/main-daaff4f95c13923f49a5.js" async=""></script><script src="/_next/static/chunks/pages/_app-041b1432faef8bf5f62a.js" async=""></script><script src="/_next/static/chunks/b38cc82cec06c0ea8b096def674947a423bb2764.260eb1ac110a53311620.js" async=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-1c9a30b06d2f3be2ac9c.js" async=""></script><script src="/_next/static/UBjnWsN6EELCOa3hLp-cA/_buildManifest.js" async=""></script><script src="/_next/static/UBjnWsN6EELCOa3hLp-cA/_ssgManifest.js" async=""></script></body></html>