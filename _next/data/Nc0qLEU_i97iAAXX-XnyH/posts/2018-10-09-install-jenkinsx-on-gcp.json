{"pageProps":{"post":{"title":"Installing Jenkins X on Google Cloud Platform","content":"<!-- markdown-link-check-disable -->\n\n<h3 id=\"introduction\">Introduction</h3>\n<p>Here are my notes while deploying Jenkins X on a Kubernetes cluster on GCP.</p>\n<p>The commands in this page have been tested on host &quot;nemo&quot; (Ubuntu 18.04.1 LTS 64-bit).</p>\n<h3 id=\"references\">References</h3>\n<ul>\n<li><a href=\"https://jenkins-x.io/getting-started/\">https://jenkins-x.io/getting-started/</a></li>\n</ul>\n<h3 id=\"create-cluster-on-gcp\">Create cluster on GCP</h3>\n<!-- 2018-10-09 09:47 CEST -->\n\n<p>Reference: <a href=\"https://jenkins-x.io/getting-started/create-cluster/\">https://jenkins-x.io/getting-started/create-cluster/</a></p>\n<p>Browse Google Cloud Console at <a href=\"https://console.cloud.google.com/\">https://console.cloud.google.com/</a></p>\n<p>Select GCP project &quot;kubernetes-workshop-218213&quot;, then click on the &quot;Activate Cloud Shell&quot; icon.</p>\n<p>Logged as <code>gmacario@cloudshell</code>, install the <code>jx</code> binary:</p>\n<pre><code class=\"language-shell\">mkdir -p ~/.jx/bin\ncurl -L https://github.com/jenkins-x/jx/releases/download/v1.3.414/jx-linux-amd64.tar.gz | tar xzv -C ~/.jx/bin\nexport PATH=$PATH:~/.jx/bin\necho &#39;export PATH=$PATH:~/.jx/bin&#39; &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>\n<p>Now use the <a href=\"https://jenkins-x.io/commands/jx_create_cluster_gke\">jx create cluster gke</a> command.</p>\n<p>You can see a demo of this command here: <a href=\"https://jenkins-x.io/demos/create_cluster_gke/\">https://jenkins-x.io/demos/create_cluster_gke/</a></p>\n<pre><code class=\"language-shell\">jx create cluster gke --skip-login\n</code></pre>\n<p>Result:</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx create cluster gke --skip-login\n? Missing required dependencies, deselect to avoid auto installing: helm\nDownloading https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz to /home/gmacario/.jx/bin/helm.tgz...\nDownloaded /home/gmacario/.jx/bin/helm.tgz\nUsing helmBinary helm with feature flag: none\n? Google Cloud Project: kubernetes-workshop-218213\nUpdated property [core/project].\nLet&#39;s ensure we have container and compute enabled on your project via: gcloud services enable container compute\nOperation &quot;operations/acf.e25f2f16-c2a5-41db-b5cb-3a93e2dac508&quot; finished successfully.\nNo cluster name provided so using a generated one: tonguetree\n? Google Cloud Zone: europe-west1-b\n? Google Cloud Machine Type: n1-standard-2\n? Minimum number of Nodes 3\n? Maximum number of Nodes 5\nCreating cluster...Initialising cluster ...\nUsing helmBinary helm with feature flag: none\nStoring the kubernetes provider gke in the TeamSettings\nUpdated the team settings in namespace jx\n? Please enter the name you wish to use with git:  Gianpaolo Macario\n? Please enter the email address you wish to use with git:  gmacario@gmail.com\nGit configured for user: Gianpaolo Macario and email gmacario@gmail.com\nTrying to create ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding for role: cluster-admin for user gmacario@gmail.com\n: clusterrolebindings.rbac.authorization.k8s.io &quot;gmacario-gmail-com-cluster-admin-binding&quot; not foundCreated ClusterRoleBinding gmacario-gmail-com-cluster-admin-binding\nUsing helm2\nConfiguring tiller\nCreated ServiceAccount tiller in namespace kube-system\nTrying to create ClusterRoleBinding tiller for role: cluster-admin and ServiceAccount: kube-system/tiller\nCreated ClusterRoleBinding tiller\nInitialising helm using ServiceAccount tiller in namespace kube-system\nUsing helmBinary helm with feature flag: none\nhelm installed and configured\n? No existing ingress controller found in the kube-system namespace, shall we install one? Yes\nInstalling using helm binary: helm\nWaiting for external loadbalancer to be created and update the nginx-ingress-controller service in kube-system namespace\nNote: this loadbalancer will fail to be provisioned if you have insufficient quotas, this can happen easily on a GKE free account. To view quotas run: gcloud compute project-info describe\nExternal loadbalancer created\nWaiting to find the external host name of the ingress controller Service in namespace kube-system with name jxing-nginx-ingress-controller\nYou can now configure a wildcard DNS pointing to the new loadbalancer address 35.241.213.226\n\nIf you do not have a custom domain setup yet, Ingress rules will be set for magic dns nip.io.\nOnce you have a customer domain ready, you can update with the command jx upgrade ingress --cluster\nIf you don&#39;t have a wildcard DNS setup then setup a new CNAME and point it at: 35.241.213.226.nip.io then use the DNS domain in the next input...? Domain 35.241.213.226.nip.io\nnginx ingress controller installed and configured\nLets set up a Git username and API token to be able to perform CI/CD\n\n? GitHub user name: gmacario\nTo be able to create a repository on GitHub we need an API Token\nPlease click this URL https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo\n\nThen COPY the token and enter in into the form below:\n\n? API Token:\n</code></pre>\n<p>Browse <a href=\"https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo\">https://github.com/settings/tokens/new?scopes=repo,read:user,read:org,user:email,write:repo_hook,delete_repo</a></p>\n<p>then paste the API Token into the terminal</p>\n<pre><code>? API Token: ****************************************\nUpdated the team settings in namespace jx\nCloning the Jenkins X cloud environments repo to /home/gmacario/.jx/cloud-environments\nEnumerating objects: 44, done.\nCounting objects: 100% (44/44), done.\nCompressing objects: 100% (32/32), done.\nTotal 1001 (delta 19), reused 33 (delta 12), pack-reused 957\nGenerated helm values /home/gmacario/.jx/extraValues.yaml\nInstalling Jenkins X platform helm chart from: /home/gmacario/.jx/cloud-environments/env-gke\nInstalling jx into namespace jx\nwaiting for install to be ready, if this is the first time then it will take a while to download images\nJenkins X deployments ready in namespace jx\n\n\n        ********************************************************\n\n             NOTE: Your admin password is: xxxxxx\n\n        ********************************************************\n\n\nGetting Jenkins API Token\nUsing url http://jenkins.jx.35.241.213.226.nip.io/me/configure\nUnable to automatically find API token with chromedp using URL http://jenkins.jx.35.241.213.226.nip.io/me/configure\nError: fork/exec /usr/bin/google-chrome: no such file or directory\nPlease go to http://jenkins.jx.35.241.213.226.nip.io/me/configure and click Show API Token to get your API Token\nThen COPY the token and enter in into the form below:\n\n? API Token:\n</code></pre>\n<!-- 2018-10-09 10:30 CEST -->\n\n<p>Login to <a href=\"http://jenkins.jx.35.241.213.226.nip.io/me/configure\">http://jenkins.jx.35.241.213.226.nip.io/me/configure</a> (username: admin; password: see above),\ncopy the API Token and paste it into the terminal</p>\n<pre><code>? API Token: ********************************\nCreated user admin API Token for Jenkins server jenkins.jx.35.241.213.226.nip.io at http://jenkins.jx.35.241.213.226.nip.io\nUpdating Jenkins with new external URL details http://jenkins.jx.35.241.213.226.nip.io\nCreating default staging and production environments\nUsing Git provider GitHub at https://github.com\n\n\nAbout to create repository environment-tonguetree-staging on server https://github.com with user gmacario\n\n\nCreating repository gmacario/environment-tonguetree-staging\nCreating Git repository gmacario/environment-tonguetree-staging\nPushed Git repository to https://github.com/gmacario/environment-tonguetree-staging\n\nCreated environment staging\nCreated Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-staging/\n\nNote that your first pipeline may take a few minutes to start while the necessary images get downloaded!\n\nCreating GitHub webhook for gmacario/environment-tonguetree-staging for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/\nUsing Git provider GitHub at https://github.com\n\n\nAbout to create repository environment-tonguetree-production on server https://github.com with user gmacario\n\n\nCreating repository gmacario/environment-tonguetree-production\nCreating Git repository gmacario/environment-tonguetree-production\nPushed Git repository to https://github.com/gmacario/environment-tonguetree-production\n\nCreated environment production\nCreated Jenkins Project: http://jenkins.jx.35.241.213.226.nip.io/job/gmacario/job/environment-tonguetree-production/\n\nNote that your first pipeline may take a few minutes to start while the necessary images get downloaded!\n\nCreating GitHub webhook for gmacario/environment-tonguetree-production for url http://jenkins.jx.35.241.213.226.nip.io/github-webhook/\n\nJenkins X installation completed successfully\n\n\n        ********************************************************\n\n             NOTE: Your admin password is: xxxx\n\n        ********************************************************\n\n\n\nYour Kubernetes context is now set to the namespace: jx\nTo switch back to your original namespace use: jx ns default\nFor help on switching contexts see: https://jenkins-x.io/developing/kube-context/\n\nTo import existing projects into Jenkins:       jx import\nTo create a new Spring Boot microservice:       jx create spring -d web -d actuator\nTo create a new microservice from a quickstart: jx create quickstart\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<h3 id=\"inspecting-the-products-of-jx-create-cluster-gke\">Inspecting the products of <code>jx create cluster gke</code></h3>\n<h4 id=\"kubernetes-cluster\">Kubernetes cluster</h4>\n<p>Logged as <code>gmacario@cloudshell</code></p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ kubectl cluster-info\nKubernetes master is running at https://35.195.217.164\nGLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nkubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nMetrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<h4 id=\"others\">Others</h4>\n<ul>\n<li>Jenkins instance: <a href=\"http://jenkins.jx.35.241.213.226.nip.io/\">http://jenkins.jx.35.241.213.226.nip.io/</a></li>\n<li>GitHub repository for staging environment: <a href=\"https://github.com/gmacario/environment-tonguetree-staging\">https://github.com/gmacario/environment-tonguetree-staging</a></li>\n<li>GitHub repository for production environment: <a href=\"https://github.com/gmacario/environment-tonguetree-production\">https://github.com/gmacario/environment-tonguetree-production</a></li>\n</ul>\n<h4 id=\"run-jx-diagnose-as-gmacariocloudshell\">Run <code>jx diagnose</code> as gmacario@cloudshell</h4>\n<!-- 2018-10-09 11:39 CEST -->\n\n<p>Logged in as <code>gmacario@cloudshell</code>, type the following command</p>\n<pre><code class=\"language-shell\">jx diagnose\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx diagnose\nRunning in namespace: jx\nJenkins X Version:\n Using helmBinary helm with feature flag: none\nFailed to find helm installs: failed to run &#39;helm list&#39; command in directory &#39;&#39;, output: &#39;Error: incompatible versions client[v2.11.0] server[v2.10.0]&#39;: exit status 1\nNAME               VERSION\njx                 1.3.399\nKubernetes cluster v1.9.7-gke.6\nkubectl            v1.10.7\nhelm client        v2.11.0+g2e55dbe\nhelm server        v2.10.0+g9ad53aa\ngit                git version 2.11.0\n\nJenkins X Status:\n Jenkins X checks passed for Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m. Jenkins is running at http://jenkins.jx.35.241.213.226.nip.io\n\nKubernetes PVCs:\n NAME                        STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\njenkins                     Bound     pvc-4014d9f4-cb9c-11e8-a1b0-42010a84014f   30Gi       RWO            standard       1h\njenkins-x-chartmuseum       Bound     pvc-40120cba-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\njenkins-x-docker-registry   Bound     pvc-40131c4c-cb9c-11e8-a1b0-42010a84014f   100Gi      RWO            standard       1h\njenkins-x-mongodb           Bound     pvc-4015b339-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\njenkins-x-nexus             Bound     pvc-401782ce-cb9c-11e8-a1b0-42010a84014f   8Gi        RWO            standard       1h\n\nKubernetes Pods:\n NAME                                            READY     STATUS    RESTARTS   AGE\njenkins-6d89bdd984-kgdrk                        1/1       Running   0          1h\njenkins-x-chartmuseum-645d78c798-9frf6          1/1       Running   0          1h\njenkins-x-controllerteam-858ff8c6b8-5vvjq       1/1       Running   0          1h\njenkins-x-controllerworkflow-6fcb699cd6-d4khj   1/1       Running   0          1h\njenkins-x-docker-registry-dcb6d6d44-dsz4n       1/1       Running   0          1h\njenkins-x-heapster-96bd95dcf-g27x4              2/2       Running   0          1h\njenkins-x-mongodb-968b595dd-rk5qs               1/1       Running   1          1h\njenkins-x-monocular-api-745c8dcd5f-kr6tg        1/1       Running   5          1h\njenkins-x-monocular-prerender-6d8897856-nlln5   1/1       Running   0          1h\njenkins-x-monocular-ui-7854f96776-njl6g         1/1       Running   0          1h\njenkins-x-nexus-55f87888dc-h5s4h                1/1       Running   0          1h\n\nKubernetes Ingresses:\n NAME              HOSTS                                      ADDRESS         PORTS     AGE\nchartmuseum       chartmuseum.jx.35.241.213.226.nip.io       35.205.100.81   80        1h\ndocker-registry   docker-registry.jx.35.241.213.226.nip.io   35.205.100.81   80        1h\njenkins           jenkins.jx.35.241.213.226.nip.io           35.205.100.81   80        1h\nmonocular         monocular.jx.35.241.213.226.nip.io         35.205.100.81   80        1h\nnexus             nexus.jx.35.241.213.226.nip.io             35.205.100.81   80        1h\n\nKubernetes Secrets:\n NAME                                       TYPE                                  DATA      AGE\ncleanup-token-8dflb                        kubernetes.io/service-account-token   3         1h\ndefault-token-qdwxp                        kubernetes.io/service-account-token   3         1h\nexpose-token-x8pt5                         kubernetes.io/service-account-token   3         1h\njenkins                                    Opaque                                3         1h\njenkins-docker-cfg                         Opaque                                1         1h\njenkins-git-credentials                    Opaque                                1         1h\njenkins-git-ssh                            Opaque                                2         1h\njenkins-hub-api-token                      Opaque                                1         1h\njenkins-maven-settings                     Opaque                                1         1h\njenkins-npm-token                          Opaque                                1         1h\njenkins-release-gpg                        Opaque                                4         1h\njenkins-ssh-config                         Opaque                                1         1h\njenkins-token-pt9cx                        kubernetes.io/service-account-token   3         1h\njenkins-x-chartmuseum                      Opaque                                2         1h\njenkins-x-controllerteam-token-zvn89       kubernetes.io/service-account-token   3         1h\njenkins-x-controllerworkflow-token-hwr86   kubernetes.io/service-account-token   3         1h\njenkins-x-docker-registry-secret           Opaque                                1         1h\njenkins-x-mongodb                          Opaque                                1         1h\njx-basic-auth                              Opaque                                1         1h\njx-install-config                          Opaque                                3         1h\njx-pipeline-git-github-github              Opaque                                2         1h\nnexus                                      Opaque                                1         1h\n\nPlease visit https://jenkins-x.io/faq/issues/ for any known issues.\nFinished printing diagnostic information.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$:\n</code></pre>\n<h4 id=\"run-jx-compliance-as-gmacariocloudshell\">Run <code>jx compliance</code> as gmacario@cloudshell</h4>\n<!-- 2018-10-09 11:42 CEST -->\n\n<p>Logged in as <code>gmacario@cloudshell</code>, type the following command</p>\n<pre><code class=\"language-shell\">jx compliance\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance\nRun compliance tests against Kubernetes cluster\n\nAvailable Commands:\n  compliance delete  Deletes the Kubernetes resources allocated by the compliance tests\n  compliance logs    Prints the logs of compliance tests\n  compliance results Shows the results of compliance tests\n  compliance run     Runs the compliance tests\n  compliance status  Retrieves the status of compliance tests\nUsage:\n  jx compliance ACTION [flags] [options]\nUse &quot;jx &lt;command&gt; --help&quot; for more information about a given command.\nUse &quot;jx options&quot; for a list of global command-line options (applies to all commands).\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<!-- 2018-10-09 11:44 CEST -->\n\n<p>Command</p>\n<pre><code class=\"language-shell\">jx compliance run\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance run\nINFO[0000] created object                                name=heptio-sonobuoy namespace= resource=namespaces\nINFO[0000] created object                                name=sonobuoy-serviceaccount namespace=heptio-sonobuoy resource=serviceaccounts\nINFO[0000] created object                                name=sonobuoy-serviceaccount-heptio-sonobuoy namespace= resource=clusterrolebindings\nINFO[0000] created object                                name=sonobuoy-serviceaccount namespace= resource=clusterroles\nINFO[0000] created object                                name=sonobuoy-config-cm namespace=heptio-sonobuoy resource=configmaps\nINFO[0000] created object                                name=sonobuoy-plugins-cm namespace=heptio-sonobuoy resource=configmaps\nINFO[0000] created object                                name=sonobuoy namespace=heptio-sonobuoy resource=pods\nINFO[0000] created object                                name=sonobuoy-master namespace=heptio-sonobuoy resource=services\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<p>Command</p>\n<pre><code class=\"language-shell\">jx compliance status\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status\nCompliance tests are still running, it can take up to 60 minutes.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<!-- 2018-10-09 13:45 CEST -->\n\n<p>After about 1h</p>\n<pre><code class=\"language-shell\">jx compliance status\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance status\nCompliance tests completed. Use `jx compliance results` to display the results.\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<p>Command</p>\n<pre><code class=\"language-shell\">jx compliance results\n</code></pre>\n<p>Result</p>\n<pre><code>gmacario@cloudshell:~ (kubernetes-workshop-218213)$ jx compliance results\nSTATUS TEST\nFAILED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image&#39;s default arguments (docker cmd) [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image&#39;s default command (docker entrypoint) [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should be able to override the image&#39;s default command and arguments [NodeConformance] [Conformance]\nPASSED [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]\nPASSED [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]\nPASSED [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should be updated [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]\nPASSED [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should *not* be restarted with a exec &quot;cat /tmp/health&quot; liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should be restarted with a exec &quot;cat /tmp/health&quot; liveness probe [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container should have monotonically increasing restart count [Slow][NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]\nPASSED [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow substituting values in a container&#39;s args [NodeConformance] [Conformance]\nPASSED [k8s.io] Variable Expansion should allow substituting values in a container&#39;s command [NodeConformance] [Conformance]\nPASSED [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]\nPASSED [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed  [Conformance]\nPASSED [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]\nPASSED [sig-api-machinery] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]\nPASSED [sig-api-machinery] Downward API should provide container&#39;s limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide host IP as an env var [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]\nPASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]\nPASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]\nPASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]\nPASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that&#39;s waiting for dependents to be deleted [Conformance]\nPASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]\nPASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]\nPASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]\nPASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]\nPASSED [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]\nPASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]\nPASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]\nPASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]\nPASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]\nPASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]\nPASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]\nPASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]\nPASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]\nPASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]\nPASSED [sig-apps] Deployment deployment should support rollover [Conformance]\nPASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]\nPASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]\nPASSED [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]\nPASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]\nPASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]\nPASSED [sig-network] DNS should provide DNS for services  [Conformance]\nPASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [NodeConformance] [Conformance]\nPASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [NodeConformance] [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]\nPASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]\nPASSED [sig-network] Service endpoints latency should not be very high  [Conformance]\nPASSED [sig-network] Services should provide secure master service  [Conformance]\nPASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]\nPASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]\nPASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]\nPASSED [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]\nPASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container&#39;s cpu limit [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container&#39;s cpu request [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container&#39;s memory limit [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide container&#39;s memory request [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should set DefaultMode on files [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should set mode on item file [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]\nPASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [NodeConformance] [Conformance]\nPASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [NodeConformance] [Conformance]\n...\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]\nPASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]\ngmacario@cloudshell:~ (kubernetes-workshop-218213)$\n</code></pre>\n<h3 id=\"control-cluster-from-nemo\">Control cluster from nemo</h3>\n<p>Browse <a href=\"https://console.cloud.google.com\">https://console.cloud.google.com</a> &gt; Kubernetes Engine &gt; Clusters</p>\n<ul>\n<li>Select cluster &quot;tonguetree&quot; &gt; Details<ul>\n<li>Take note of Endpoint: 35.195.217.164</li>\n</ul>\n</li>\n<li>Click &quot;Show cluster certificate&quot;<ul>\n<li>Take note of Cluster CA certificate: ...</li>\n</ul>\n</li>\n</ul>\n<p>Reference: <a href=\"https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials\">https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials</a></p>\n<h4 id=\"list-kubernetes-clusters-on-gcp\">List Kubernetes clusters on GCP</h4>\n<p>Logged as <code>gpmacario@nemo</code></p>\n<pre><code class=\"language-shell\">gcloud container clusters list\n</code></pre>\n<p>Result:</p>\n<pre><code>gpmacario@nemo:~ $ gcloud container clusters list\nNAME        LOCATION        MASTER_VERSION  MASTER_IP       MACHINE_TYPE   NODE_VERSION  NUM_NODES  STATUS\nkube-101    europe-west1-b  1.10.7-gke.2    35.241.146.224  n1-standard-1  1.10.7-gke.2  2          RUNNING\ntonguetree  europe-west1-b  1.9.7-gke.6     35.195.217.164  n1-standard-2  1.9.7-gke.6   3          RUNNING\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"get-credentials-for-gke-cluster-tonguetree\">Get credentials for GKE cluster &quot;tonguetree&quot;</h4>\n<p>Logged as gpmacario@nemo</p>\n<pre><code class=\"language-shell\">gcloud container clusters get-credentials tonguetree\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ gcloud container clusters get-credentials tonguetree\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for tonguetree.\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-kubectl-cluster-info\">run <code>kubectl cluster-info</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">kubectl cluster-info\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ kubectl cluster-info\nKubernetes master is running at https://35.195.217.164\nGLBCDefaultBackend is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nkubernetes-dashboard is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nMetrics-server is running at https://35.195.217.164/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\n\nTo further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-kubectl-get-nodes\">Run <code>kubectl get nodes</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">kubectl get nodes\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE   VERSION\ngke-tonguetree-default-pool-5c0fe7ba-t50s   Ready    &lt;none&gt;   68m   v1.9.7-gke.6\ngke-tonguetree-default-pool-5c0fe7ba-x7mw   Ready    &lt;none&gt;   68m   v1.9.7-gke.6\ngke-tonguetree-default-pool-5c0fe7ba-xj5l   Ready    &lt;none&gt;   68m   v1.9.7-gke.6\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-kubectl-get-namespaces\">Run <code>kubectl get namespaces</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">kubectl get namespaces\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ kubectl get namespaces\nNAME            STATUS   AGE\ndefault         Active   75m\njx              Active   74m\njx-production   Active   52m\njx-staging      Active   52m\nkube-public     Active   75m\nkube-system     Active   75m\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-kubectl-get-services--n-jx\">Run <code>kubectl get services -n jx</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">kubectl get services -n jx\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ kubectl get services -n jx\nNAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE\nheapster                        ClusterIP   10.27.247.74    &lt;none&gt;        8082/TCP    63m\njenkins                         ClusterIP   10.27.249.0     &lt;none&gt;        8080/TCP    63m\njenkins-agent                   ClusterIP   10.27.253.40    &lt;none&gt;        50000/TCP   63m\njenkins-x-chartmuseum           ClusterIP   10.27.240.69    &lt;none&gt;        8080/TCP    63m\njenkins-x-docker-registry       ClusterIP   10.27.252.143   &lt;none&gt;        5000/TCP    63m\njenkins-x-mongodb               ClusterIP   10.27.251.85    &lt;none&gt;        27017/TCP   63m\njenkins-x-monocular-api         ClusterIP   10.27.241.218   &lt;none&gt;        80/TCP      63m\njenkins-x-monocular-prerender   ClusterIP   10.27.243.227   &lt;none&gt;        80/TCP      63m\njenkins-x-monocular-ui          ClusterIP   10.27.243.130   &lt;none&gt;        80/TCP      63m\nnexus                           ClusterIP   10.27.243.12    &lt;none&gt;        80/TCP      63m\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-kubectl-proxy\">Run <code>kubectl proxy</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">kubectl proxy\n</code></pre>\n<pre><code>gpmacario@nemo:~ $ kubectl proxy\nStarting to serve on 127.0.0.1:8001\n</code></pre>\n<h4 id=\"display-kubernetes-dashboard\">Display Kubernetes dashboard</h4>\n<p>Logged as <code>gpmacario@nemo</code>,\nbrowse <a href=\"http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\">http://127.0.0.1:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a></p>\n<blockquote>\n<p><strong>Kubeconfig</strong></p>\n<p>Please select the kubeconfig file that you have created to configure access to the cluster.\nTo find out more about how to configure and use kubeconfig file, please refer to the\n<a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/\">Configure Access to Multiple Clusters</a> section.</p>\n<p><strong>Token</strong></p>\n<p>Every Service Account has a Secret with valid Bearer Token that can be used to log in to Dashboard.\nTo find out more about how to configure and use Bearer Tokens, please refer to the\n<a href=\"https://kubernetes.io/docs/admin/authentication/\">Authentication</a> section.</p>\n</blockquote>\n<p>Select &quot;Token&quot;, then paste the result of the following command</p>\n<pre><code class=\"language-shell\">kubectl config view | grep access-token\n</code></pre>\n<p>and click &quot;SIGN IN&quot;.</p>\n<h3 id=\"trying-jx-commands-from-nemo\">Trying <code>jx</code> commands from nemo</h3>\n<h4 id=\"install-the-jx-binary\">Install the <code>jx</code> binary</h4>\n<p>Reference: <a href=\"https://jenkins-x.io/getting-started/install/\">https://jenkins-x.io/getting-started/install/</a></p>\n<p>Logged as gpmacario@nemo</p>\n<pre><code class=\"language-shell\">mkdir -p ~/Downloads &amp;&amp; cd ~/Downloads\ncurl -L https://github.com/jenkins-x/jx/releases/download/v1.3.399/jx-linux-amd64.tar.gz | tar xzv\nsudo mv jx /usr/local/bin\n</code></pre>\n<h4 id=\"display-jx-help\">Display jx help</h4>\n<pre><code>gpmacario@nemo:~ $ jx\n\n\nInstalling:\n  install                   Install Jenkins X in the current Kubernetes cluster\n  uninstall                 Uninstall the Jenkins X platform\n  upgrade                   Upgrades a resource\n  create cluster            Create a new Kubernetes cluster\n  update cluster            Updates an existing Kubernetes cluster\n  create jenkins token      Adds a new username and API token for a Jenkins server\n  init                      Init Jenkins X\n\nAdding Projects to Jenkins X:\n  import                    Imports a local project or Git repository into Jenkins\n  create archetype          Create a new app from a Maven Archetype and import the generated code into Git and Jenkins for CI/CD\n  create spring             Create a new Spring Boot application and import the generated code into Git and Jenkins for CI/CD\n  create lile               Create a new Lile based application and import the generated code into Git and Jenkins for CI/CD\n  create micro              Create a new micro based application and import the generated code into Git and Jenkins for CI/CD\n  create quickstart         Create a new app from a Quickstart and import the generated code into Git and Jenkins for CI/CD\n  create quickstartlocation Create a location of quickstarts for your team\n\nAddons:\n  create addon              Creates an addon\n  create token addon        Adds a new token/login for a user for a given addon\n  delete addon              Deletes one or more addons\n  delete token addon        Deletes one or more API tokens for a user on an issue addon server\n\nGit:\n  create git server         Creates a new Git server URL\n\n\nWorking with CloudBees application:\n  cloudbees                 Opens the CloudBees app for Kubernetes for visualising CI/CD and your environments\n  login                     Onboard an user into the CloudBees application\n\nWorking with Environments:\n  preview                   Creates or updates a Preview Environment for the current version of an application\n  promote                   Promotes a version of an application to an Environment\n  create environment        Create a new Environment which is used to promote your Team&#39;s Applications via Continuous Delivery\n  delete environment        Deletes one or more Environments\n  edit environment          Edits an Environment which is used to promote your Team&#39;s Applications via Continuous Delivery\n  get environments          Display one or more Environments\n\nWorking with Jenkins X resources:\n  get                       Display one or more resources\n  edit                      Edit a resource\n  create                    Create a new resource\n  update                    Updates an existing resource\n  delete                    Deletes one or more resources\n  start                     Starts a process such as a pipeline\n  stop                      Stops a process such as a pipeline\n\nJenkins X Pipeline Commands:\n  step                      pipeline steps\n\nJenkins X services:\n  controller                Runs a controller\n  gc                        Garbage collects Jenkins X resources\n\nOther Commands:\n  diagnose                  Print diagnostic information about the Jenkins X installation\n  docs                      Open the documentation in a browser\n  help                      Help about any command\n  version                   Print the version information\nOptions:\n      --version=false: version for jx\nUsage:\n  jx [flags] [options]\nUse &quot;jx &lt;command&gt; --help&quot; for more information about a given command.\nUse &quot;jx options&quot; for a list of global command-line options (applies to all commands).\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-jx---version\">Run <code>jx --version</code></h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">jx --version\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ jx --version\n1.3.399\ngpmacario@nemo:~ $\n</code></pre>\n<h4 id=\"run-jx-diagnose-as-gpmacarionemo\">Run <code>jx diagnose</code> as gpmacario@nemo</h4>\n<p>Command</p>\n<pre><code class=\"language-shell\">jx diagnose\n</code></pre>\n<p>Result</p>\n<pre><code>gpmacario@nemo:~ $ jx diagnose\nRunning in namespace: default\nJenkins X Version:\n Using helmBinary helm with feature flag: none\nNAME               VERSION\njx                 1.3.399\njenkins x platform 0.0.2755\nKubernetes cluster v1.9.7-gke.6\nkubectl            v1.12.1\nhelm client        v2.9.1+g20adb27\nhelm server        v2.10.0+g9ad53aa\ngit                git version 2.17.1\nUnable to get the Jenkins X Status\nerror: Command failed &#39;jx status&#39;: Unable to find JX components in Cluster(gke_kubernetes-workshop-218213_europe-west1-b_tonguetree): 3 nodes, memory 14% of 17354292Ki, cpu 37% of 5790m\nyou could try:   # Default installer which uses interactive prompts to generate git secrets\n  jx install\n\n  # Install with a GitHub personal access token\n  jx install --git-username jenkins-x-bot --git-api-token 9fdbd2d070cd81eb12bca87861bcd850\n\n  # If you know the cloud provider you can pass this as a CLI argument. E.g. for AWS\n  jx install --provider=aws\n\nInstalls the Jenkins X platform on a Kubernetes cluster\n\nRequires a --git-username and --git-api-token that can be used to create a new token. This is so the Jenkins X platform can git tag your releases\n\nFor more documentation see: https://jenkins-x.io/getting-started/install-on-cluster/\n\nThe current requirements are:\n\n *RBAC is enabled on the cluster\n\n *Insecure Docker registry is enabled for Docker registries running locally inside Kubernetes on the service IP range. See the above documentation for more detailerror: no deployments found in namespace default exit status 1\ngpmacario@nemo:~ $\n</code></pre>\n<h3 id=\"summary\">Summary</h3>\n<p>This post explained how to set up a Jenkins X instance on Google Cloud Platform.</p>\n<p>Stay tuned for future posts on the subject!</p>\n<h3 id=\"see-also\">See also</h3>\n<ul>\n<li><a href=\"https://jenkins-x.io/\">https://jenkins-x.io/</a></li>\n<li><a href=\"https://jenkins.io/projects/jenkins-x/\">https://jenkins.io/projects/jenkins-x/</a></li>\n</ul>\n<!-- markdown-link-check-enable -->\n<!-- EOF -->\n"},"config":{"title":"gmacario.github.io","description":"Gianpaolo Macario's public rants","social":{"github":{"name":"gmacario","link":"https://github.com/gmacario"},"twitter":{"name":"gpmacario","link":"https://www.twitter.com/gpmacario"}}}},"__N_SSG":true}